{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed559af",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Python, NumPy, PyTorch 모두 동일하게 고정\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # multi-GPU일 경우\n",
    "\n",
    "# CuDNN 연산에서 완전 재현성 보장\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c9fb4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_csv(path):\n",
    "    with open(path, 'r', encoding='utf-8-sig') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"날짜 시간\" in line:\n",
    "            header_idx = i\n",
    "            break\n",
    "\n",
    "    if header_idx is None:\n",
    "        raise ValueError(f\"'날짜 시간' 행을 찾을 수 없습니다: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, skiprows=header_idx)\n",
    "    df['날짜 시간'] = pd.to_datetime(df['날짜 시간'])\n",
    "    df['날짜'] = df['날짜 시간'].dt.date\n",
    "\n",
    "    # ---- 클로로필 관련 열 자동 탐색 ----\n",
    "    chl_cols = [c for c in df.columns if \"Chlorophyll\" in c and \"농도\" in c]\n",
    "\n",
    "    for col in chl_cols:\n",
    "        if \"(ppb)\" in col:\n",
    "            # ppb → µg/L 변환\n",
    "            df[col.replace(\"(ppb)\", \"(µg/L)\")] = df[col]\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cd69eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_point_folder(point_folder, save_dir):\n",
    "    \"\"\"하나의 포인트 폴더 내 모든 날짜 CSV를 불러와 평균 요약\"\"\"\n",
    "    summary_list = []\n",
    "\n",
    "    for date_folder in sorted(os.listdir(point_folder)):\n",
    "        date_path = os.path.join(point_folder, date_folder)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue  # 혹시 폴더 아닌게 섞여 있을 수도 있음\n",
    "\n",
    "        daily_dfs = []\n",
    "\n",
    "        for file in os.listdir(date_path):\n",
    "            if not file.endswith(\".csv\"):\n",
    "                continue\n",
    "            file_path = os.path.join(date_path, file)\n",
    "            try:\n",
    "                df = load_clean_csv(file_path)\n",
    "                daily_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ {file_path} 로드 실패: {e}\")\n",
    "\n",
    "        if len(daily_dfs) == 0:\n",
    "            continue\n",
    "\n",
    "        # 여러 CSV를 concat 후 평균 계산\n",
    "        daily_df = pd.concat(daily_dfs)\n",
    "        daily_mean = daily_df.mean(numeric_only=True)\n",
    "        daily_mean[\"날짜\"] = pd.to_datetime(date_folder, format=\"%Y%m%d\").date()\n",
    "\n",
    "        summary_list.append(daily_mean)\n",
    "\n",
    "    # 날짜별 평균이 담긴 DataFrame 생성\n",
    "    if len(summary_list) > 0:\n",
    "        summary_df = pd.DataFrame(summary_list)\n",
    "        summary_df.sort_values(\"날짜\", inplace=True)\n",
    "        cols = [\"날짜\"] + [c for c in summary_df.columns if c != \"날짜\"]\n",
    "        summary_df = summary_df[cols]\n",
    "        \n",
    "        point_name = os.path.basename(point_folder.rstrip(\"/\\\\\"))\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"{point_name}_summary.csv\")\n",
    "        summary_df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ {point_name} 요약 CSV 저장 완료 → {save_path}\")\n",
    "    else:\n",
    "        print(f\"❌ {point_folder} 요약 데이터 없음.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cd92b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n",
      "point_data\\포인트1\\20250402\\VuSitu_라이브_리딩_2025-04-02_11-07-40_장치_위치.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"point_data\"\n",
    "dataset = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "52c8ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: point_data\\포인트1\\20250402\\VuSitu_라이브_리딩_2025-04-02_11-07-40_장치_위치.csv\n",
      "데이터 shape: (69, 26)\n",
      "컬럼명: ['날짜 시간', 'Actual Conductivity (µS/cm) (1066342)', 'Specific Conductivity (µS/cm) (1066342)', '염도 (PSU) (1066342)', '저항력 (Ω⋅cm) (1066342)', '밀도 (g/cm³) (1066342)', '총 용존 고형물 (TDS) (ppt) (1066342)', 'pH (pH) (1057024)', 'pH mV (mV) (1057024)', 'ORP (mV) (1057024)', '온도 (°C) (1068851)', '압력 (psi) (1064654)', '깊이 (m) (1064654)', '표면 고도 (m) (1064654)', 'RDO 농도 (mg/L) (954526)', 'RDO 포화 (%Sat) (954526)', '산소분압 (Torr) (954526)', 'Chlorophyll-a Fluorescence (RFU) (945787)', 'Chlorophyll-a 농도 (µg/L) (945787)', '외부 전압 (V) (1068851)', '대기압 (mbar) (1049665)', '온도 (°C) (1049665)', '위도 (°)', '경도 (°)', '표시됨', '날짜']\n"
     ]
    }
   ],
   "source": [
    "sample_path = dataset[0]\n",
    "df = load_clean_csv(sample_path)\n",
    "\n",
    "print(\"파일 경로:\", sample_path)\n",
    "print(\"데이터 shape:\", df.shape)\n",
    "print(\"컬럼명:\", df.columns.tolist())\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "855279cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 49 49\n"
     ]
    }
   ],
   "source": [
    "data_num = len(dataset)\n",
    "train_dataset, val_dataset, test_dataset = dataset[:int(data_num*0.8)], dataset[int(data_num*0.8):int(data_num*0.9)], dataset[int(data_num*0.9):]\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "652b0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_sequences(df_point, input_days=7):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(df_point) - input_days):\n",
    "#         X.append(df_point.iloc[i:i+input_days].values)\n",
    "#         y.append(df_point.iloc[i+input_days][\"chlorophyll\"])  # 다음날 예측\n",
    "#     return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f9c279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        input_size: 입력 feature 수\n",
    "        hidden_size: LSTM hidden 차원\n",
    "        num_layers: LSTM 층 수\n",
    "        output_size: 예측값 차원\n",
    "        \"\"\"\n",
    "        super(VanillaLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, input_size]\n",
    "        \"\"\"\n",
    "        out, _ = self.lstm(x)       # out: [batch, seq_len, hidden_size]\n",
    "        out = out[:, -1, :]          # 마지막 시점 출력\n",
    "        out = self.fc(out)           # [batch, output_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0e33b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c13afdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 포인트1 요약 CSV 저장 완료 → summaries\\포인트1_summary.csv\n",
      "✅ 포인트2 요약 CSV 저장 완료 → summaries\\포인트2_summary.csv\n",
      "✅ 포인트3 요약 CSV 저장 완료 → summaries\\포인트3_summary.csv\n",
      "✅ 포인트4 요약 CSV 저장 완료 → summaries\\포인트4_summary.csv\n",
      "✅ 포인트5 요약 CSV 저장 완료 → summaries\\포인트5_summary.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"point_data\"      # 포인트 폴더들이 들어 있는 상위 폴더\n",
    "output_dir = \"summaries\"      # 요약본 저장 폴더\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for point_folder in sorted(os.listdir(dataset_root)):\n",
    "    full_path = os.path.join(dataset_root, point_folder)\n",
    "    if os.path.isdir(full_path):\n",
    "        summarize_point_folder(full_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "caa7d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (32, 23)\n",
      "X shape: (25, 7, 23), y shape: (25, 1)\n",
      "25 samples in dataset\n",
      "Train size: 20, Val size: 2, Test size: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "df = pd.read_csv(\"summaries/포인트3_summary.csv\")\n",
    "\n",
    "# 날짜 제거 (index로 쓰거나 feature에서 제외)\n",
    "df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "df = df.sort_values('날짜')\n",
    "features = df.drop(columns=['날짜', '표시됨']).values\n",
    "print(\"Features shape:\", features.shape)\n",
    "\n",
    "# Chlorophyll-a 농도 컬럼 인덱스 (features 기준)\n",
    "chl_idx = 17\n",
    "\n",
    "# 정규화 (LSTM은 scale 민감)\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "seq_len = 7\n",
    "X = np.array([features[i:i+seq_len] for i in range(len(features) - seq_len)])\n",
    "y = features[seq_len:, chl_idx:chl_idx+1]  # shape: [num_samples, 1]\n",
    "dataset = TimeSeriesDataset(X, y)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(len(dataset), \"samples in dataset\")\n",
    "\n",
    "# n_total = len(dataset)\n",
    "# n_train = int(n_total * 0.8)\n",
    "# n_val = int(n_total * 0.1)\n",
    "# n_test = n_total - n_train - n_val\n",
    "# train_set, val_set, test_set = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = int(len(dataset) * 0.1)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set = Subset(dataset, range(0, train_size))\n",
    "val_set = Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_set = Subset(dataset, range(train_size + val_size, len(dataset)))\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}, Test size: {len(test_set)}\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8)\n",
    "test_loader = DataLoader(test_set, batch_size=8)\n",
    "\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "model = VanillaLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7c146513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"🔍 NaN 개수:\", np.isnan(features).sum())\n",
    "# print(\"🔍 inf 개수:\", np.isinf(features).sum())\n",
    "# print(\"🔍 각 컬럼별 최소/최대:\")\n",
    "# print(pd.DataFrame(features).agg(['min', 'max']).T.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b0fa7ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] Train Loss: 0.029866 | Val Loss: 0.203518\n",
      "Epoch [2/300] Train Loss: 0.021571 | Val Loss: 0.142465\n",
      "Epoch [3/300] Train Loss: 0.019911 | Val Loss: 0.133916\n",
      "Epoch [4/300] Train Loss: 0.016714 | Val Loss: 0.145448\n",
      "Epoch [5/300] Train Loss: 0.013943 | Val Loss: 0.152807\n",
      "Epoch [6/300] Train Loss: 0.012683 | Val Loss: 0.147141\n",
      "Epoch [7/300] Train Loss: 0.014270 | Val Loss: 0.130941\n",
      "Epoch [8/300] Train Loss: 0.010192 | Val Loss: 0.094041\n",
      "Epoch [9/300] Train Loss: 0.006988 | Val Loss: 0.065543\n",
      "Epoch [10/300] Train Loss: 0.004711 | Val Loss: 0.043997\n",
      "Epoch [11/300] Train Loss: 0.005817 | Val Loss: 0.027584\n",
      "Epoch [12/300] Train Loss: 0.004339 | Val Loss: 0.013116\n",
      "Epoch [13/300] Train Loss: 0.006618 | Val Loss: 0.016286\n",
      "Epoch [14/300] Train Loss: 0.003513 | Val Loss: 0.044777\n",
      "Epoch [15/300] Train Loss: 0.003759 | Val Loss: 0.054657\n",
      "Epoch [16/300] Train Loss: 0.002898 | Val Loss: 0.053664\n",
      "Epoch [17/300] Train Loss: 0.003118 | Val Loss: 0.056774\n",
      "Epoch [18/300] Train Loss: 0.003331 | Val Loss: 0.068172\n",
      "Epoch [19/300] Train Loss: 0.002606 | Val Loss: 0.064084\n",
      "Epoch [20/300] Train Loss: 0.002324 | Val Loss: 0.055722\n",
      "Epoch [21/300] Train Loss: 0.003189 | Val Loss: 0.052506\n",
      "Epoch [22/300] Train Loss: 0.001921 | Val Loss: 0.052252\n",
      "Epoch [23/300] Train Loss: 0.002726 | Val Loss: 0.049105\n",
      "Epoch [24/300] Train Loss: 0.002418 | Val Loss: 0.042974\n",
      "Epoch [25/300] Train Loss: 0.003213 | Val Loss: 0.047915\n",
      "Epoch [26/300] Train Loss: 0.002103 | Val Loss: 0.057679\n",
      "Epoch [27/300] Train Loss: 0.001842 | Val Loss: 0.054704\n",
      "Epoch [28/300] Train Loss: 0.001677 | Val Loss: 0.052448\n",
      "Epoch [29/300] Train Loss: 0.002600 | Val Loss: 0.057776\n",
      "Epoch [30/300] Train Loss: 0.001736 | Val Loss: 0.067096\n",
      "Epoch [31/300] Train Loss: 0.002940 | Val Loss: 0.060056\n",
      "Epoch [32/300] Train Loss: 0.001704 | Val Loss: 0.056899\n",
      "Epoch [33/300] Train Loss: 0.002449 | Val Loss: 0.062653\n",
      "Epoch [34/300] Train Loss: 0.002312 | Val Loss: 0.066870\n",
      "Epoch [35/300] Train Loss: 0.001766 | Val Loss: 0.056491\n",
      "Epoch [36/300] Train Loss: 0.002044 | Val Loss: 0.055899\n",
      "Epoch [37/300] Train Loss: 0.002067 | Val Loss: 0.058326\n",
      "Epoch [38/300] Train Loss: 0.001681 | Val Loss: 0.057647\n",
      "Epoch [39/300] Train Loss: 0.001587 | Val Loss: 0.060581\n",
      "Epoch [40/300] Train Loss: 0.001554 | Val Loss: 0.062767\n",
      "Epoch [41/300] Train Loss: 0.001580 | Val Loss: 0.065863\n",
      "Epoch [42/300] Train Loss: 0.001556 | Val Loss: 0.064663\n",
      "Epoch [43/300] Train Loss: 0.001775 | Val Loss: 0.065907\n",
      "Epoch [44/300] Train Loss: 0.001697 | Val Loss: 0.060719\n",
      "Epoch [45/300] Train Loss: 0.001526 | Val Loss: 0.067458\n",
      "Epoch [46/300] Train Loss: 0.001569 | Val Loss: 0.069105\n",
      "Epoch [47/300] Train Loss: 0.001617 | Val Loss: 0.067839\n",
      "Epoch [48/300] Train Loss: 0.002439 | Val Loss: 0.064136\n",
      "Epoch [49/300] Train Loss: 0.001476 | Val Loss: 0.074231\n",
      "Epoch [50/300] Train Loss: 0.001596 | Val Loss: 0.068833\n",
      "Epoch [51/300] Train Loss: 0.001596 | Val Loss: 0.067826\n",
      "Epoch [52/300] Train Loss: 0.001789 | Val Loss: 0.075584\n",
      "Epoch [53/300] Train Loss: 0.001500 | Val Loss: 0.068032\n",
      "Epoch [54/300] Train Loss: 0.001580 | Val Loss: 0.064916\n",
      "Epoch [55/300] Train Loss: 0.002498 | Val Loss: 0.073696\n",
      "Epoch [56/300] Train Loss: 0.002513 | Val Loss: 0.072537\n",
      "Epoch [57/300] Train Loss: 0.001526 | Val Loss: 0.070719\n",
      "Epoch [58/300] Train Loss: 0.001808 | Val Loss: 0.079513\n",
      "Epoch [59/300] Train Loss: 0.002274 | Val Loss: 0.072738\n",
      "Epoch [60/300] Train Loss: 0.002009 | Val Loss: 0.083705\n",
      "Epoch [61/300] Train Loss: 0.001622 | Val Loss: 0.081406\n",
      "Epoch [62/300] Train Loss: 0.001793 | Val Loss: 0.069501\n",
      "Epoch [63/300] Train Loss: 0.001641 | Val Loss: 0.082815\n",
      "Epoch [64/300] Train Loss: 0.001895 | Val Loss: 0.084383\n",
      "Epoch [65/300] Train Loss: 0.002015 | Val Loss: 0.081221\n",
      "Epoch [66/300] Train Loss: 0.001577 | Val Loss: 0.072057\n",
      "Epoch [67/300] Train Loss: 0.001659 | Val Loss: 0.085755\n",
      "Epoch [68/300] Train Loss: 0.001331 | Val Loss: 0.082023\n",
      "Epoch [69/300] Train Loss: 0.001755 | Val Loss: 0.079825\n",
      "Epoch [70/300] Train Loss: 0.002323 | Val Loss: 0.080646\n",
      "Epoch [71/300] Train Loss: 0.001462 | Val Loss: 0.089900\n",
      "Epoch [72/300] Train Loss: 0.001277 | Val Loss: 0.090116\n",
      "Epoch [73/300] Train Loss: 0.001327 | Val Loss: 0.090121\n",
      "Epoch [74/300] Train Loss: 0.001380 | Val Loss: 0.099978\n",
      "Epoch [75/300] Train Loss: 0.001279 | Val Loss: 0.098680\n",
      "Epoch [76/300] Train Loss: 0.001877 | Val Loss: 0.100809\n",
      "Epoch [77/300] Train Loss: 0.001203 | Val Loss: 0.100107\n",
      "Epoch [78/300] Train Loss: 0.001312 | Val Loss: 0.094066\n",
      "Epoch [79/300] Train Loss: 0.001318 | Val Loss: 0.093648\n",
      "Epoch [80/300] Train Loss: 0.001229 | Val Loss: 0.099821\n",
      "Epoch [81/300] Train Loss: 0.001465 | Val Loss: 0.100458\n",
      "Epoch [82/300] Train Loss: 0.001277 | Val Loss: 0.095039\n",
      "Epoch [83/300] Train Loss: 0.001255 | Val Loss: 0.109993\n",
      "Epoch [84/300] Train Loss: 0.001366 | Val Loss: 0.109033\n",
      "Epoch [85/300] Train Loss: 0.001169 | Val Loss: 0.097876\n",
      "Epoch [86/300] Train Loss: 0.001320 | Val Loss: 0.110440\n",
      "Epoch [87/300] Train Loss: 0.001726 | Val Loss: 0.118640\n",
      "Epoch [88/300] Train Loss: 0.001384 | Val Loss: 0.103628\n",
      "Epoch [89/300] Train Loss: 0.001400 | Val Loss: 0.103088\n",
      "Epoch [90/300] Train Loss: 0.001662 | Val Loss: 0.118493\n",
      "Epoch [91/300] Train Loss: 0.001340 | Val Loss: 0.103826\n",
      "Epoch [92/300] Train Loss: 0.002245 | Val Loss: 0.106221\n",
      "Epoch [93/300] Train Loss: 0.001363 | Val Loss: 0.131811\n",
      "Epoch [94/300] Train Loss: 0.001798 | Val Loss: 0.116805\n",
      "Epoch [95/300] Train Loss: 0.001343 | Val Loss: 0.104149\n",
      "Epoch [96/300] Train Loss: 0.002001 | Val Loss: 0.110350\n",
      "Epoch [97/300] Train Loss: 0.001929 | Val Loss: 0.130723\n",
      "Epoch [98/300] Train Loss: 0.002151 | Val Loss: 0.105640\n",
      "Epoch [99/300] Train Loss: 0.001575 | Val Loss: 0.115109\n",
      "Epoch [100/300] Train Loss: 0.001066 | Val Loss: 0.113757\n",
      "Epoch [101/300] Train Loss: 0.001077 | Val Loss: 0.120165\n",
      "Epoch [102/300] Train Loss: 0.000981 | Val Loss: 0.122027\n",
      "Epoch [103/300] Train Loss: 0.001060 | Val Loss: 0.119592\n",
      "Epoch [104/300] Train Loss: 0.000978 | Val Loss: 0.125081\n",
      "Epoch [105/300] Train Loss: 0.001485 | Val Loss: 0.128676\n",
      "Epoch [106/300] Train Loss: 0.000954 | Val Loss: 0.124466\n",
      "Epoch [107/300] Train Loss: 0.001494 | Val Loss: 0.120203\n",
      "Epoch [108/300] Train Loss: 0.001131 | Val Loss: 0.126125\n",
      "Epoch [109/300] Train Loss: 0.001268 | Val Loss: 0.111747\n",
      "Epoch [110/300] Train Loss: 0.001665 | Val Loss: 0.118735\n",
      "Epoch [111/300] Train Loss: 0.000821 | Val Loss: 0.140883\n",
      "Epoch [112/300] Train Loss: 0.001515 | Val Loss: 0.125919\n",
      "Epoch [113/300] Train Loss: 0.001384 | Val Loss: 0.127220\n",
      "Epoch [114/300] Train Loss: 0.000982 | Val Loss: 0.135934\n",
      "Epoch [115/300] Train Loss: 0.001225 | Val Loss: 0.123181\n",
      "Epoch [116/300] Train Loss: 0.001193 | Val Loss: 0.128111\n",
      "Epoch [117/300] Train Loss: 0.000966 | Val Loss: 0.126191\n",
      "Epoch [118/300] Train Loss: 0.001366 | Val Loss: 0.136790\n",
      "Epoch [119/300] Train Loss: 0.001139 | Val Loss: 0.147101\n",
      "Epoch [120/300] Train Loss: 0.000860 | Val Loss: 0.128418\n",
      "Epoch [121/300] Train Loss: 0.001013 | Val Loss: 0.138096\n",
      "Epoch [122/300] Train Loss: 0.001127 | Val Loss: 0.147620\n",
      "Epoch [123/300] Train Loss: 0.000895 | Val Loss: 0.133240\n",
      "Epoch [124/300] Train Loss: 0.001039 | Val Loss: 0.131068\n",
      "Epoch [125/300] Train Loss: 0.001045 | Val Loss: 0.131330\n",
      "Epoch [126/300] Train Loss: 0.000785 | Val Loss: 0.152232\n",
      "Epoch [127/300] Train Loss: 0.001625 | Val Loss: 0.140296\n",
      "Epoch [128/300] Train Loss: 0.001369 | Val Loss: 0.119952\n",
      "Epoch [129/300] Train Loss: 0.000987 | Val Loss: 0.141349\n",
      "Epoch [130/300] Train Loss: 0.001100 | Val Loss: 0.125688\n",
      "Epoch [131/300] Train Loss: 0.000950 | Val Loss: 0.117892\n",
      "Epoch [132/300] Train Loss: 0.000980 | Val Loss: 0.144884\n",
      "Epoch [133/300] Train Loss: 0.001336 | Val Loss: 0.140319\n",
      "Epoch [134/300] Train Loss: 0.001088 | Val Loss: 0.120830\n",
      "Epoch [135/300] Train Loss: 0.001177 | Val Loss: 0.144982\n",
      "Epoch [136/300] Train Loss: 0.000952 | Val Loss: 0.133166\n",
      "Epoch [137/300] Train Loss: 0.000923 | Val Loss: 0.121824\n",
      "Epoch [138/300] Train Loss: 0.001126 | Val Loss: 0.131039\n",
      "Epoch [139/300] Train Loss: 0.001720 | Val Loss: 0.137921\n",
      "Epoch [140/300] Train Loss: 0.001534 | Val Loss: 0.118585\n",
      "Epoch [141/300] Train Loss: 0.001040 | Val Loss: 0.150812\n",
      "Epoch [142/300] Train Loss: 0.001145 | Val Loss: 0.143809\n",
      "Epoch [143/300] Train Loss: 0.000713 | Val Loss: 0.126901\n",
      "Epoch [144/300] Train Loss: 0.000930 | Val Loss: 0.137294\n",
      "Epoch [145/300] Train Loss: 0.000590 | Val Loss: 0.130983\n",
      "Epoch [146/300] Train Loss: 0.000720 | Val Loss: 0.130829\n",
      "Epoch [147/300] Train Loss: 0.000921 | Val Loss: 0.130540\n",
      "Epoch [148/300] Train Loss: 0.000812 | Val Loss: 0.114750\n",
      "Epoch [149/300] Train Loss: 0.001176 | Val Loss: 0.131010\n",
      "Epoch [150/300] Train Loss: 0.000553 | Val Loss: 0.120960\n",
      "Epoch [151/300] Train Loss: 0.000702 | Val Loss: 0.132987\n",
      "Epoch [152/300] Train Loss: 0.000552 | Val Loss: 0.143266\n",
      "Epoch [153/300] Train Loss: 0.000739 | Val Loss: 0.130048\n",
      "Epoch [154/300] Train Loss: 0.000567 | Val Loss: 0.123152\n",
      "Epoch [155/300] Train Loss: 0.000638 | Val Loss: 0.125468\n",
      "Epoch [156/300] Train Loss: 0.000463 | Val Loss: 0.136603\n",
      "Epoch [157/300] Train Loss: 0.000565 | Val Loss: 0.129166\n",
      "Epoch [158/300] Train Loss: 0.000402 | Val Loss: 0.138118\n",
      "Epoch [159/300] Train Loss: 0.000657 | Val Loss: 0.136400\n",
      "Epoch [160/300] Train Loss: 0.000502 | Val Loss: 0.126170\n",
      "Epoch [161/300] Train Loss: 0.000510 | Val Loss: 0.133804\n",
      "Epoch [162/300] Train Loss: 0.000392 | Val Loss: 0.126583\n",
      "Epoch [163/300] Train Loss: 0.000488 | Val Loss: 0.132304\n",
      "Epoch [164/300] Train Loss: 0.000401 | Val Loss: 0.141163\n",
      "Epoch [165/300] Train Loss: 0.000413 | Val Loss: 0.131847\n",
      "Epoch [166/300] Train Loss: 0.000453 | Val Loss: 0.136306\n",
      "Epoch [167/300] Train Loss: 0.000529 | Val Loss: 0.138458\n",
      "Epoch [168/300] Train Loss: 0.000310 | Val Loss: 0.122426\n",
      "Epoch [169/300] Train Loss: 0.000480 | Val Loss: 0.133079\n",
      "Epoch [170/300] Train Loss: 0.000582 | Val Loss: 0.134104\n",
      "Epoch [171/300] Train Loss: 0.000396 | Val Loss: 0.117861\n",
      "Epoch [172/300] Train Loss: 0.000483 | Val Loss: 0.132412\n",
      "Epoch [173/300] Train Loss: 0.000350 | Val Loss: 0.131428\n",
      "Epoch [174/300] Train Loss: 0.000439 | Val Loss: 0.127427\n",
      "Epoch [175/300] Train Loss: 0.000428 | Val Loss: 0.126089\n",
      "Epoch [176/300] Train Loss: 0.000310 | Val Loss: 0.134355\n",
      "Epoch [177/300] Train Loss: 0.000408 | Val Loss: 0.120480\n",
      "Epoch [178/300] Train Loss: 0.000460 | Val Loss: 0.126324\n",
      "Epoch [179/300] Train Loss: 0.000331 | Val Loss: 0.141677\n",
      "Epoch [180/300] Train Loss: 0.000410 | Val Loss: 0.125531\n",
      "Epoch [181/300] Train Loss: 0.000319 | Val Loss: 0.136021\n",
      "Epoch [182/300] Train Loss: 0.000294 | Val Loss: 0.135245\n",
      "Epoch [183/300] Train Loss: 0.000223 | Val Loss: 0.123434\n",
      "Epoch [184/300] Train Loss: 0.000284 | Val Loss: 0.129157\n",
      "Epoch [185/300] Train Loss: 0.000369 | Val Loss: 0.126773\n",
      "Epoch [186/300] Train Loss: 0.000400 | Val Loss: 0.130015\n",
      "Epoch [187/300] Train Loss: 0.000559 | Val Loss: 0.140179\n",
      "Epoch [188/300] Train Loss: 0.000243 | Val Loss: 0.131529\n",
      "Epoch [189/300] Train Loss: 0.000321 | Val Loss: 0.145353\n",
      "Epoch [190/300] Train Loss: 0.000345 | Val Loss: 0.128299\n",
      "Epoch [191/300] Train Loss: 0.000338 | Val Loss: 0.131792\n",
      "Epoch [192/300] Train Loss: 0.000200 | Val Loss: 0.133803\n",
      "Epoch [193/300] Train Loss: 0.000114 | Val Loss: 0.125044\n",
      "Epoch [194/300] Train Loss: 0.000195 | Val Loss: 0.133665\n",
      "Epoch [195/300] Train Loss: 0.000257 | Val Loss: 0.131469\n",
      "Epoch [196/300] Train Loss: 0.000178 | Val Loss: 0.123267\n",
      "Epoch [197/300] Train Loss: 0.000173 | Val Loss: 0.137032\n",
      "Epoch [198/300] Train Loss: 0.000348 | Val Loss: 0.135531\n",
      "Epoch [199/300] Train Loss: 0.000153 | Val Loss: 0.126671\n",
      "Epoch [200/300] Train Loss: 0.000283 | Val Loss: 0.134006\n",
      "Epoch [201/300] Train Loss: 0.000283 | Val Loss: 0.127789\n",
      "Epoch [202/300] Train Loss: 0.000161 | Val Loss: 0.123224\n",
      "Epoch [203/300] Train Loss: 0.000188 | Val Loss: 0.127008\n",
      "Epoch [204/300] Train Loss: 0.000198 | Val Loss: 0.123494\n",
      "Epoch [205/300] Train Loss: 0.000143 | Val Loss: 0.128456\n",
      "Epoch [206/300] Train Loss: 0.000204 | Val Loss: 0.128373\n",
      "Epoch [207/300] Train Loss: 0.000146 | Val Loss: 0.126790\n",
      "Epoch [208/300] Train Loss: 0.000162 | Val Loss: 0.131117\n",
      "Epoch [209/300] Train Loss: 0.000194 | Val Loss: 0.119762\n",
      "Epoch [210/300] Train Loss: 0.000265 | Val Loss: 0.128446\n",
      "Epoch [211/300] Train Loss: 0.000317 | Val Loss: 0.129922\n",
      "Epoch [212/300] Train Loss: 0.000119 | Val Loss: 0.127954\n",
      "Epoch [213/300] Train Loss: 0.000245 | Val Loss: 0.130617\n",
      "Epoch [214/300] Train Loss: 0.000177 | Val Loss: 0.126973\n",
      "Epoch [215/300] Train Loss: 0.000146 | Val Loss: 0.136437\n",
      "Epoch [216/300] Train Loss: 0.000193 | Val Loss: 0.125570\n",
      "Epoch [217/300] Train Loss: 0.000173 | Val Loss: 0.131092\n",
      "Epoch [218/300] Train Loss: 0.000092 | Val Loss: 0.126426\n",
      "Epoch [219/300] Train Loss: 0.000125 | Val Loss: 0.135275\n",
      "Epoch [220/300] Train Loss: 0.000130 | Val Loss: 0.130858\n",
      "Epoch [221/300] Train Loss: 0.000138 | Val Loss: 0.132781\n",
      "Epoch [222/300] Train Loss: 0.000101 | Val Loss: 0.134167\n",
      "Epoch [223/300] Train Loss: 0.000103 | Val Loss: 0.125265\n",
      "Epoch [224/300] Train Loss: 0.000130 | Val Loss: 0.139993\n",
      "Epoch [225/300] Train Loss: 0.000123 | Val Loss: 0.129846\n",
      "Epoch [226/300] Train Loss: 0.000130 | Val Loss: 0.139320\n",
      "Epoch [227/300] Train Loss: 0.000171 | Val Loss: 0.130786\n",
      "Epoch [228/300] Train Loss: 0.000150 | Val Loss: 0.131643\n",
      "Epoch [229/300] Train Loss: 0.000125 | Val Loss: 0.134462\n",
      "Epoch [230/300] Train Loss: 0.000098 | Val Loss: 0.124380\n",
      "Epoch [231/300] Train Loss: 0.000207 | Val Loss: 0.132001\n",
      "Epoch [232/300] Train Loss: 0.000173 | Val Loss: 0.127942\n",
      "Epoch [233/300] Train Loss: 0.000139 | Val Loss: 0.127784\n",
      "Epoch [234/300] Train Loss: 0.000161 | Val Loss: 0.122117\n",
      "Epoch [235/300] Train Loss: 0.000116 | Val Loss: 0.126022\n",
      "Epoch [236/300] Train Loss: 0.000063 | Val Loss: 0.118815\n",
      "Epoch [237/300] Train Loss: 0.000129 | Val Loss: 0.129529\n",
      "Epoch [238/300] Train Loss: 0.000128 | Val Loss: 0.123713\n",
      "Epoch [239/300] Train Loss: 0.000191 | Val Loss: 0.130117\n",
      "Epoch [240/300] Train Loss: 0.000298 | Val Loss: 0.125192\n",
      "Epoch [241/300] Train Loss: 0.000222 | Val Loss: 0.122087\n",
      "Epoch [242/300] Train Loss: 0.000118 | Val Loss: 0.140930\n",
      "Epoch [243/300] Train Loss: 0.000364 | Val Loss: 0.116172\n",
      "Epoch [244/300] Train Loss: 0.000234 | Val Loss: 0.127308\n",
      "Epoch [245/300] Train Loss: 0.000293 | Val Loss: 0.120647\n",
      "Epoch [246/300] Train Loss: 0.000151 | Val Loss: 0.124099\n",
      "Epoch [247/300] Train Loss: 0.000153 | Val Loss: 0.130885\n",
      "Epoch [248/300] Train Loss: 0.000167 | Val Loss: 0.116301\n",
      "Epoch [249/300] Train Loss: 0.000161 | Val Loss: 0.133194\n",
      "Epoch [250/300] Train Loss: 0.000163 | Val Loss: 0.129967\n",
      "Epoch [251/300] Train Loss: 0.000121 | Val Loss: 0.128864\n",
      "Epoch [252/300] Train Loss: 0.000171 | Val Loss: 0.137975\n",
      "Epoch [253/300] Train Loss: 0.000188 | Val Loss: 0.121449\n",
      "Epoch [254/300] Train Loss: 0.000194 | Val Loss: 0.122049\n",
      "Epoch [255/300] Train Loss: 0.000203 | Val Loss: 0.117842\n",
      "Epoch [256/300] Train Loss: 0.000215 | Val Loss: 0.135271\n",
      "Epoch [257/300] Train Loss: 0.000191 | Val Loss: 0.129424\n",
      "Epoch [258/300] Train Loss: 0.000184 | Val Loss: 0.132414\n",
      "Epoch [259/300] Train Loss: 0.000093 | Val Loss: 0.121841\n",
      "Epoch [260/300] Train Loss: 0.000098 | Val Loss: 0.121179\n",
      "Epoch [261/300] Train Loss: 0.000129 | Val Loss: 0.129048\n",
      "Epoch [262/300] Train Loss: 0.000083 | Val Loss: 0.126112\n",
      "Epoch [263/300] Train Loss: 0.000089 | Val Loss: 0.131702\n",
      "Epoch [264/300] Train Loss: 0.000094 | Val Loss: 0.124754\n",
      "Epoch [265/300] Train Loss: 0.000059 | Val Loss: 0.124464\n",
      "Epoch [266/300] Train Loss: 0.000109 | Val Loss: 0.124696\n",
      "Epoch [267/300] Train Loss: 0.000122 | Val Loss: 0.124148\n",
      "Epoch [268/300] Train Loss: 0.000070 | Val Loss: 0.129437\n",
      "Epoch [269/300] Train Loss: 0.000131 | Val Loss: 0.124063\n",
      "Epoch [270/300] Train Loss: 0.000186 | Val Loss: 0.123766\n",
      "Epoch [271/300] Train Loss: 0.000256 | Val Loss: 0.124684\n",
      "Epoch [272/300] Train Loss: 0.000192 | Val Loss: 0.111181\n",
      "Epoch [273/300] Train Loss: 0.000227 | Val Loss: 0.137110\n",
      "Epoch [274/300] Train Loss: 0.000398 | Val Loss: 0.114062\n",
      "Epoch [275/300] Train Loss: 0.000837 | Val Loss: 0.133480\n",
      "Epoch [276/300] Train Loss: 0.000830 | Val Loss: 0.112724\n",
      "Epoch [277/300] Train Loss: 0.000461 | Val Loss: 0.104693\n",
      "Epoch [278/300] Train Loss: 0.000492 | Val Loss: 0.117286\n",
      "Epoch [279/300] Train Loss: 0.000291 | Val Loss: 0.104440\n",
      "Epoch [280/300] Train Loss: 0.000208 | Val Loss: 0.122738\n",
      "Epoch [281/300] Train Loss: 0.000268 | Val Loss: 0.113436\n",
      "Epoch [282/300] Train Loss: 0.000113 | Val Loss: 0.105302\n",
      "Epoch [283/300] Train Loss: 0.000071 | Val Loss: 0.114182\n",
      "Epoch [284/300] Train Loss: 0.000207 | Val Loss: 0.105607\n",
      "Epoch [285/300] Train Loss: 0.000283 | Val Loss: 0.111845\n",
      "Epoch [286/300] Train Loss: 0.000193 | Val Loss: 0.122330\n",
      "Epoch [287/300] Train Loss: 0.000186 | Val Loss: 0.099327\n",
      "Epoch [288/300] Train Loss: 0.000341 | Val Loss: 0.111946\n",
      "Epoch [289/300] Train Loss: 0.000297 | Val Loss: 0.112439\n",
      "Epoch [290/300] Train Loss: 0.000171 | Val Loss: 0.104307\n",
      "Epoch [291/300] Train Loss: 0.000185 | Val Loss: 0.124078\n",
      "Epoch [292/300] Train Loss: 0.000397 | Val Loss: 0.111405\n",
      "Epoch [293/300] Train Loss: 0.000175 | Val Loss: 0.104490\n",
      "Epoch [294/300] Train Loss: 0.000268 | Val Loss: 0.112968\n",
      "Epoch [295/300] Train Loss: 0.000139 | Val Loss: 0.106990\n",
      "Epoch [296/300] Train Loss: 0.000134 | Val Loss: 0.108281\n",
      "Epoch [297/300] Train Loss: 0.000206 | Val Loss: 0.110841\n",
      "Epoch [298/300] Train Loss: 0.000065 | Val Loss: 0.113590\n",
      "Epoch [299/300] Train Loss: 0.000118 | Val Loss: 0.115371\n",
      "Epoch [300/300] Train Loss: 0.000076 | Val Loss: 0.115624\n",
      "✅ 12 model loaded with Val Loss: 0.013116\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            preds = model(X_val)\n",
    "            val_loss += criterion(preds, y_val).item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {total_loss/len(train_loader):.6f} | Val Loss: {val_loss/len(val_loader):.6f}\")\\\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"✅ {best_epoch} model loaded with Val Loss: {best_val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ad82d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor([[0.5296],\n",
      "        [0.4328],\n",
      "        [0.8899]])\n",
      "preds: tensor([[0.2122],\n",
      "        [0.1108],\n",
      "        [0.0896]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.2816, RMSE: 0.5307\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test = X_test.float()\n",
    "        y_test = y_test.float()\n",
    "\n",
    "        y_preds = model(X_test)\n",
    "        all_preds.append(y_preds.cpu())\n",
    "        all_labels.append(y_test.cpu())\n",
    "        test_loss += criterion(y_preds, y_test).item()\n",
    "\n",
    "print(\"label:\", y_test)\n",
    "print(\"preds:\", y_preds)\n",
    "\n",
    "y_pred = torch.cat(all_preds, dim=0).numpy().flatten()\n",
    "y_true = torch.cat(all_labels, dim=0).numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # y=x 기준선\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Prediction vs Actual Scatter\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f\"Test MSE: {mse:.4f}, RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label: tensor([[0.5296],\n",
    "#         [0.4328],\n",
    "#         [0.8899]])\n",
    "# preds: tensor([[0.2836],\n",
    "#         [0.2120],\n",
    "#         [0.1964]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
